{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Module 3 lab - KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have discussed clustering in Statmath course. You can [revisit the course here](https://jupyterhub.dsa.missouri.edu/user/skaf48/tree/DSA-8610/modules/module7). As an unsupervised data analysis technique, It organises data objects by proximity based on its variables and helps to create natural groupings for a set of data objects. By grouping data one can understand how each data point relates to each other and discover groups of similar ones. \n",
    "\n",
    "When the groups are formed, centroids can be defined for them, the ideal data object that minimises the sum of the distances to each of the data points in a cluster. By analysing these centroids variables we will be able to define each cluster in terms of its characteristics.\n",
    "\n",
    "In this notebook, the dataset is related to deaths from Infectious Tuberculosis disease in each country from 1990 to 2007. The data is available on [gapminder website](http://www.gapminder.org/data/) which is a comprehensive resource for data regarding different countries and territories indicators. We will work with data where each sample is a country and each variable is a year. \n",
    "\n",
    "Kmeans clustering divides set of data objects into k clusters, assigning each observation to a cluster so as to minimize the distance of that observation (in n-dimensional space) to the cluster’s mean; the means are then recomputed. This operation is run iteratively until the clusters converge, for a maximum of number of iterations chosen. Given a target number, k, of clusters to find, it will locate the centers of each of those k clusters and the boundaries between them. It does this using the following algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with a randomly selected set of  k  centroids (the supposed centers of the  k  clusters)\n",
    "2. Determine which observation is in which cluster, based on which centroid it is closest to (using the squared Euclidean distance:  ∑pj=1(xij−xi′j)2  where  p  is the number of dimensions)\n",
    "3. Re-calculate the centroids of each cluster by minimizing the squared Euclidean distance to each observation in the cluster\n",
    "4. Repeat 2. and 3. until the members of the clusters (and hence the positions of the centroids) no longer change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tb_deaths_per_100k = pd.read_csv(\"../../../datasets/TB/tb_deaths_per_100k.csv\",index_col = 0, thousands  = ',')\n",
    "tb_deaths_per_100k.index.names = ['country']\n",
    "tb_deaths_per_100k.columns.names = ['year']\n",
    "tb_deaths_per_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extra row of NANs' is introduced. Get rid of that row using pandas indexing as shown below. Also get rid of the extra column \"Unnames: 19\" that is introduced at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude last row of data\n",
    "tb_deaths_per_100k = tb_deaths_per_100k[:-1]\n",
    "\n",
    "# Exclude last column with NANs'\n",
    "del tb_deaths_per_100k[\"Unnamed: 19\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb_deaths_per_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans takes set of data points and clusters them in specified number of groups. The data here has 18 variables. Use PCA to reduce the dimensions of the data. Python's sklearn machine learning library comes with a PCA implementation. \n",
    "\n",
    "For very large dimensional data, normal PCA implementation may not work. We should consider apache Spark's dimensionality reduction features that is explored in Database Analytics course last semester. In this case, there are just 18 variables. Its a small feature set for today's machine learning libraries and computer capabilities. So we are ok. Specify in advance the number of principal components we want to use. Call the fit() method on the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(tb_deaths_per_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths_pca = pca.transform(tb_deaths_per_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe using the two principal components generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the two principal components generated. \n",
    "deaths_pca_df = pd.DataFrame(deaths_pca)\n",
    "\n",
    "# Assign the indexes of tb_deaths_per_100 to deaths_pca_df. tb_deaths_per_100 has country names as its indexes. So they both\n",
    "# will have same indexes now.\n",
    "deaths_pca_df.index = tb_deaths_per_100k.index\n",
    "\n",
    "# Rename the column names of deaths_pca_df to something meaningful. \n",
    "deaths_pca_df.columns = ['PC1','PC2']\n",
    "deaths_pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at explained variance ratio by each principal component as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component explains over 90% of the variance, while the second one accounts for nearly 6% for a total of almost 96% between the two of them.\n",
    "\n",
    "Now that a lower dimensionality version of data is ready, call plot function on the data frame, by passing the kind of plot you want and what columns correspond to each axis. The for loop below adds an annotation to tag country with a point.\n",
    "\n",
    "**Reference: ** [enumerate()](http://book.pythontips.com/en/latest/enumerate.html): It allows to loop over a list and have an automatic counter and value for corresponding to . Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = deaths_pca_df.plot(kind='scatter', x='PC2', y='PC1', figsize=(16,8))\n",
    "\n",
    "for i, country in enumerate(deaths_pca_df.index):\n",
    "    ax.annotate(country, (deaths_pca_df.iloc[i].PC2, deaths_pca_df.iloc[i].PC1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bubble chart, by setting the point size to a value proportional to the mean value for all the years in that particular country. First of all, a new column containing the re-scaled mean per country across all the years shuld be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "deaths_pca_df['country_mean'] = pd.Series(tb_deaths_per_100k.mean(axis=1), index=tb_deaths_per_100k.index)\n",
    "country_mean_max = deaths_pca_df['country_mean'].max()\n",
    "country_mean_min = deaths_pca_df['country_mean'].min()\n",
    "country_mean_scaled = (deaths_pca_df.country_mean-country_mean_min) / country_mean_max\n",
    "\n",
    "deaths_pca_df['country_mean_scaled'] = pd.Series(country_mean_scaled, index=deaths_pca_df.index) \n",
    "deaths_pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deaths_pca_df.plot(kind='scatter', x='PC2', y='PC1', s=deaths_pca_df['country_mean_scaled']*100, figsize=(16,8), c=deaths_pca_df['country_mean_scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plots suggest that most variation happens along the y axis which is PC1. At the bottom of the chart there is concentration of countries that are mostly developed. When ascending the axis, the number of countries is becoming more sparse and they belong to less developed regions of the world.\n",
    "\n",
    "When the points are coloured and sized using the magnitude average, the directions also correspond to a variation in these magnitudes.\n",
    "\n",
    "PCA did tell us something about the data. How it is varying over all in the dataset. WE still dont know the relationships between countries. We will use k-means clustering to group countries based on how similar their situation has been year-by-year. Then we will use cluster assignment to colour code previous scatter plot that we generated.\n",
    "\n",
    "When using k-means, the most important thing to do id to determine the right number of group for the data. This can be done more or less accurately by iterating through different values for the number of groups and compare an amount called the within-cluster sum of square distances for each iteration. This is the squared sum of distances to the cluster center for each cluster member. Of course this distance is minimal when the number of clusters gets equal to the number of samples, but we don't want to get there. We normally stop when the improvement in this value starts decreasing at a lower rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### silhouette score\n",
    "\n",
    "Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object lies within its cluster - from wikipedia.org. The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. It will tell us whats is the optimum number of clsuters for given data. \n",
    "\n",
    "**Reference: **\n",
    "- [sklearn.metrics.silhouette_score¶](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "- [Silhouette (clustering)](https://en.wikipedia.org/wiki/Silhouette_(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "s = []\n",
    "for n in range(2,30):\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(deaths_pca_df)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    s.append(silhouette_score(deaths_pca_df, labels, metric='euclidean'))\n",
    "\n",
    "plt.plot(s)\n",
    "plt.ylabel(\"Silouette\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"Silouette for K-means cell's behaviour\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette ranges from -1 to 1. A high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. So we need to choose an optimum value. Lets start with 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit(tb_deaths_per_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the cluster assignments together with each country in data frame. The cluster labels are returned in clusters.labels_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths_pca_df['cluster'] = pd.Series(clusters.labels_, index=deaths_pca_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to plot, using the cluster column as color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ggplot import *\n",
    "\n",
    "# deaths_pca_df.plot(kind='scatter', x='PC2',y='PC1',c=deaths_pca_df.cluster.astype(np.float), figsize=(16,8))\n",
    "\n",
    "# for i, country in enumerate(deaths_pca_df.index):\n",
    "#     ax.annotate(cluster, (deaths_pca_df.iloc[i].PC2, deaths_pca_df.iloc[i].PC1))\n",
    "    \n",
    "ggplot(deaths_pca_df, aes(x='PC2', y='PC1', color='cluster')) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab = deaths_pca_df.groupby(['cluster']).size()\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "clusters = kmeans.fit(tb_deaths_per_100k)\n",
    "\n",
    "deaths_pca_df['cluster'] = pd.Series(clusters.labels_, index=deaths_pca_df.index)\n",
    "\n",
    "ggplot(deaths_pca_df, aes(x='PC2', y='PC1', color='cluster')) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab = deaths_pca_df.groupby(['cluster']).size()\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "clusters = kmeans.fit(tb_deaths_per_100k)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "deaths_pca_df['cluster'] = pd.Series(clusters.labels_, index=deaths_pca_df.index)\n",
    "\n",
    "ggplot(deaths_pca_df, aes(x='PC2', y='PC1', color='cluster')) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab = deaths_pca_df.groupby(['cluster']).size()\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpreting cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 contains 37 countries and the centroid of the cluster is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(deaths_pca_df.loc[deaths_pca_df['cluster']==0].shape)\n",
    "deaths_pca_df[deaths_pca_df['cluster'] == 0].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2 contains 143 countries and the centroid of the cluster is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(deaths_pca_df.loc[deaths_pca_df['cluster']==1].shape)\n",
    "deaths_pca_df[deaths_pca_df['cluster'] == 1].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 3 contains just 1 country and the centroid of the cluster is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(deaths_pca_df.loc[deaths_pca_df['cluster']==2].shape)\n",
    "deaths_pca_df[deaths_pca_df['cluster'] == 2].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 4 contains 23 countries and the centroid of the cluster is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(deaths_pca_df.loc[deaths_pca_df['cluster']==3].shape)\n",
    "deaths_pca_df[deaths_pca_df['cluster'] == 3].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 5 contains 3 countries and the centroid of the cluster is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(deaths_pca_df.loc[deaths_pca_df['cluster']==4].shape)\n",
    "deaths_pca_df[deaths_pca_df['cluster'] == 4].index.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
