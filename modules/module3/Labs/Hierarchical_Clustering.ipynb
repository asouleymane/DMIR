{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how Kmeans algorithm formed clusters which expects pre determined number of clusters to be formed. In this notebook we will move on to hierarchical clustering which has the benefit that one doesn't need to already know the number of clusters k in data. We will use the same old iris data to illustrate the concept in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the iris data from /datasets folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hca\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "data=pd.read_csv(\"../../../datasets/iris.txt\")\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an extra column of data. Get rid of it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "num_data = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(num_data.iloc[:,0], num_data.iloc[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = hca.linkage(data.iloc[:,:-1], 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ward`** is one of the methods that can be used to calculate the distance between newly formed clusters. 'ward' causes linkage() to use the Ward variance minimization algorithm.\n",
    "\n",
    "There are some other common linkage methods like 'single', 'complete', 'average' along with different distance metrics like 'euclidean' (default), 'Manhattan', 'hamming', 'cosine'. You can try different distance measures clustering methods to make sure that the data is not just be clustered to minimize the overall intra cluster variance in euclidean space. For example, when dealing with clustering text documents, it may not look right to use euclidean distance metric with long (binary) feature vectors (word-vectors in text clustering).\n",
    "\n",
    "Explore the linked methods and metrics to make a informed choice between different options available. \n",
    "\n",
    "**`Cophenetic Correlation Coefficient`:** Check the Cophenetic Correlation Coefficient of the clustering generated using cophenet() function. This compares (correlates) the actual pairwise distances of all data samples to those implied by the hierarchical clustering. The closer the value is to 1, the better the clustering preserves the original distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, cop_dist = cophenet(Z, pdist(num_data))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linkage() function will use the supplied method and metric to calculate the distances of the clusters starting with all the **`n`** individual data points as singleton clusters and in each iteration will merge the two clusters which have the smallest distance according the selected method and metric. \n",
    "\n",
    "It will return an array of length n - 1 giving the information about the n - 1 cluster merges which it did to pairwise merge the n singleton clusters. Z[i] will tell which clusters were merged in the i-th iteration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each row of the resulting array has the format [idx1, idx2, dist, sample_count]. Rows 101 and 142 in the dataset are clustered together to form first cluster. These two samples have 0 distance and created a cluster with a total of 2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In second iteration, rows 7 and 39 in the dataset are clustered together to form a cluster. These two samples have a distance of 0.1. Have a look at the first 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can observe the sample_count column, until iteration 22 the algorithm only directly merged original samples. Also, there has been a monotonic increase in the distance.\n",
    "\n",
    "In iteration 22 the algorithm merged cluster indices 40 with 152. Do you remember that teher are only 150 samples in the dataset meaning the original sample indices are 0 to 149 for 150 samples. All indices idx >= len(num_data) actually refer to the cluster formed in Z[idx - len(num_data)].\n",
    "\n",
    "Simply put, while idx 149 corresponds to num_data[149], idx 150 corresponds to the cluster formed in Z[0], idx 151 to Z[1], 152 to Z[2] and so on...\n",
    "\n",
    "Hence, the merge iteration 22 merged sample 40 to samples 0 and 17 that were previously merged in iteration 2 (152 - 2).\n",
    "\n",
    "Let's check out the points coordinates to see if this makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_data.loc[[0, 17, 40], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points seem pretty close. Plot the points highlighted to verify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLotting Sepal.Length vs Petal.Width\n",
    "idxs = [0, 17, 40]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(num_data.iloc[:,0], num_data.iloc[:,3])  # plot all points\n",
    "\n",
    "# Highlight the points of interest in Red color\n",
    "plt.scatter(num_data.iloc[idxs,0], num_data.iloc[idxs,3], c='r')  # plot interesting points in red again\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 3 red dots are pretty close to each other as predicted. Lets check the results of 28th iteration. The algorithm merged indices 88 to 82 and 92. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_data.loc[[88, 82, 92], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLotting Sepal.Length vs Petal.Width\n",
    "idxs = [88, 82, 92]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(num_data.iloc[:,0], num_data.iloc[:,1])  # plot all points\n",
    "\n",
    "# Highlight the points of interest in Red color\n",
    "plt.scatter(num_data.iloc[idxs,0], num_data.iloc[idxs,1], c='r')  # plot interesting points in red again\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Dendogram\n",
    "\n",
    "A dendrogram is a visualization technique that is in the form of a tree showing the order and distances of merges during the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=hca.linkage(num_data,metric=\"euclidean\",method=\"ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: ** \n",
    "- [scipy.cluster.hierarchy.linkage(y, method='single', metric='euclidean')](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage). \n",
    " \n",
    "linkage() performs hierarchical/agglomerative clustering on the condensed distance matrix y. y must be a ${n \\choose 2}$ sized vector where n is the number of original observations paired in the distance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below code block, truncate_mode='lastp' option tells python to show just the last p non-singleton clusters formed in the linkage. All other non-singleton clusters are contracted into leaf nodes. \n",
    "\n",
    "The number p indicates number of clusters to be shown. Its gives a truncated dendrogram, which only shows the last p=25 out of the 149 merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt=hca.dendrogram(y,truncate_mode=\"lastp\",p=25)\n",
    "xticks(rotation=90)\n",
    "ylabel(\"Distance\")\n",
    "figtext(0.5,0.95,\"Iris flower data set\",ha=\"center\",fontsize=12)\n",
    "figtext(0.5,0.90,\"Dendrogram (center, euclidean, ward)\",ha=\"center\",fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: **\n",
    "\n",
    "- [scipy.cluster.hierarchy.dendrogram(data,....)](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\n",
    "\n",
    "Plots the hierarchical clustering as a dendrogram. The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The height of the top of the U-link is the distance between its children clusters. It is also the cophenetic distance between original observations in the two children clusters. It is expected that the distances in data[:,2] be monotonic, otherwise crossings appear in the dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The x axis labels, if nothing is specified then they are the indices of samples in num_data.\n",
    "- The labels on y axis represent the distances (of the 'ward' method in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the dendogram:\n",
    "\n",
    "Starting from each label at the bottom, you can see a vertical line up to a horizontal line. The height of that horizontal line tells you about the distance at which this label was merged into another label or cluster. \n",
    "\n",
    "Summarizing:\n",
    "\n",
    "Horizontal lines are cluster merges. Vertical lines tell you which clusters/labels were part of merge forming that new cluster. Heights of the horizontal lines tell you about the distance that needed to be \"bridged\" to form the new cluster. You can also see that from distances > 12 up there's a huge jump of the distance to the final merge at a distance of approx 33. Let's have a look at the distances of the last 4 merges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z[-4:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such merges in the dendrogram are interesting. They indicate that something has been merged that shouldn't be merged may be. Simply put, maybe the things that were merged really don't belong to the same cluster suggesting that maybe there are just 2 clusters different clusters.\n",
    "\n",
    "The colors of the clusters come from color_threshold argument of dendrogram(). Because it is not specified here, it automatically picked a distance cut-off value of 70 % of the final merge and then colored the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Distance Cut-Off or Determining the Number of Clusters\n",
    "\n",
    "As explained above already, a huge jump in distance is typically what we're interested in when arguing for a certain number of clusters. When you have to pick the number of clusters manually, that is probably the best method as it allows to gain some insights into the data and to perform some sanity checks on the edge cases. In this case the distance cut off could be 10, as the jump is pretty obvious. \n",
    "\n",
    "When automatically picking the number of clusters there are a couple of common methods that can be used. \n",
    "\n",
    "#### Inconsistency Method\n",
    "\"inconsistency\" method is one of the defaults for the fcluster() function in scipy.\n",
    "\n",
    "The question driving the inconsistency method is \"what makes a distance jump a jump?\". It answers this by comparing each cluster merge's height h to the average avg and normalizing it by the standard deviation. Link statistics are computed over link heights for links d levels below the cluster i.\n",
    "\n",
    "$$inconsistency=\\frac{h−avg}{std}$$\n",
    "The following shows a matrix of the avg, std, count, inconsistency for each of the last 10 merges of our hierarchical clustering with depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import inconsistent\n",
    "\n",
    "depth = 5\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no paricular golden rule to pick the depth limit in the inconsistencies. See what happens if depth is set to 3 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth = 3\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inconsistency values heavily depend on the depth of the tree you calculate the averages over. \n",
    "\n",
    "#### Elbow Method\n",
    "\n",
    "It tries to find the clustering step where the acceleration of distance growth is the biggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(last)\n",
    "print(acceleration)\n",
    "print(idxs)\n",
    "print(idxs[:-2])\n",
    "print(acceleration_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take last 10 merge distances into variable last\n",
    "last = Z[-10:, 2]\n",
    "\n",
    "# reverse the elemensts in variable last and assign them to last_rev. The index '-1' starts the indexing from the last.\n",
    "last_rev = last[::-1]\n",
    "\n",
    "# create an array with indexes 1,2,3...10 i;e start from 1 to until 11. It will not include 11. \n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "\n",
    "# Plot the merge distances. last_rev has merge with highest value at index 0. \n",
    "plt.plot(idxs, last_rev)\n",
    "\n",
    "\n",
    "# Calculate the 2nd order discrete difference along given axis.\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "\n",
    "# reverse the elemensts in variable acceleration and assign them to acceleration_rev. \n",
    "acceleration_rev = acceleration[::-1]\n",
    "\n",
    "# Plot the 2nd order distances. \n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.show()\n",
    "\n",
    "# Find the index of largest value in acceleration_rev to decide the number of clusters. \n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print(\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Clusters\n",
    "\n",
    "For retrieving the clusters, for different ways of determining k the fcluster function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing max_d:\n",
    "\n",
    "Onvce the max distance is determined with help of a dendrogram, then cluster id for each of the samples can be extracted using below peice of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 50\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowing k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=2\n",
    "fcluster(Z, k, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Inconsistency Method (default):\n",
    "\n",
    "Inconsistency Method is the default method in fcluster function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters=fcluster(Z, 8, depth=10)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Your Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(num_data.iloc[:,0], num_data.iloc[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
