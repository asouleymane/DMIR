{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5 - Text Analysis\n",
    "\n",
    "\n",
    "-----\n",
    "### Introduction: \n",
    "\n",
    "In this module, lets see how to analyze text data, which is one of the most important and exciting application areas in Data Science. Text analysis is used in many pplications like text categorization, text clustering, sentiment analysis, Social media data analysis, spam filtering, mining text data from forms and many more. It forms the basis for natural language processing. We will discuss text classification and text mining over next two weeks while this notebook focuses on basic text analysis tasks such as accessing data, tokenizing a corpus, and computing token frequencies. We illustrate these tasks using functionalities in **tm** package in R.\n",
    "\n",
    "\n",
    "Twitter is a social networking and communication website founded in 2006. It is one of the Top 10 most visited sites on the internet. Users share and send messages that can be no longer than 140 characters long. People also use it as a method of notification for natural disasters and for tracking diseases. Celebrities, politicians, and companies connect with fans and customers using Twitter. Managing public perception is essential. Reacting to changing sentiment, identifying offensive posts,determining topics of interest are indeed crucial. In this notebok, we'll see how the power of analytics can be used to address these questions. We'll discuss how can we use data from Twitter and turn them into knowledge and then understand opinions and sentiment from the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Most of the data we have come across till now is structured, numerical,or categorical. But if you look at the tweets, they are loosely structured. They are often textual. They have poor spelling, often contain non-traditional grammar, and they are multilingual. The key question, is how to handle this information included in the tweets. Humans cannot keep up with this volumes of data as there are more than half a billion tweets per day. \n",
    "\n",
    "So the field of computers that addresses how computers understand text is called Natural Language Processing. Let's discuss briefly the history of Natural Language Processing. The initial focus has been on understanding grammar. Later, the focus shifted towards statistical, machine learning techniques that learn from large bodies of text. Today, there are modern versions of these Natural Language Processing. Apple is using Siri, and Google is using Google Now. \n",
    "\n",
    "Why is it hard for computers to deal with textual data?. For example, suppose if you say, \"I put my bag in the car\". Is it large and blue?. The question is, does the \"it\" refers to the bag or the car? The context is often important. Humans use homonyms, metaphors and often sarcasm. We'll see how we can build analytics models using text as our data.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll be trying to understand sentiment of tweets about the company Apple. Our challenge in this lecture is to see if we can correctly classify tweets as being negative, positive, or neither about Apple. To collect the data needed for this task,we had to perform two steps.The first was to collect data about tweets from the internet. Twitter data is publicly available. And you can collect it through scraping the website or by using a special interface for programmers that Twitter provides called an API. Then we need to construct the outcome variable for these tweets, which means that we have to label them as positive, negative, or neutral sentiment. The outcome will be one of the following -- strongly negative, negative, neutral,positive, and strongly positive. These responses are represented as a number on the scale from negative 2 to 2. So now there are a bunch of tweets that are labeled with their sentiment. \n",
    "\n",
    "We need to build independent variables from the text of a tweet to be used to predict the sentiment. \n",
    "\n",
    "-------\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Transforms text into independent variables. Did it occur to your mind, how can you classify text data made up of words when machine learning algorithms work on numerical data. The only way is to build a numerical summary of the data that the algorithms can work on. An easy approach to implement this idea is to identify all possible words in the documents and to track the number of times each words occurs. In the contect of this notebook, each tweet will be a document. This produces a (very) sparse matrix for the documents, where the columns are the possible words (or tokens) and the rows are different tweets.\n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse matrices is more formally known as bag of words, because we effectively create the bag of words out of which are documents are constructed. In this model, each document is mapped into a vector, where the individual elements in the vector correspond to the number of times the words (associated with the particular column) appears in the document.\n",
    "\n",
    "\n",
    "For example, in the sentence, \"This is a great place to eat. I would recommend this place to my friends,\" the word this is seen twice, the word place is seen twice, the word great is seen once.\n",
    "\n",
    "\n",
    "There's one feature for each word in bag of words. The data has to be pre processed like everyother data. It will dramatically improve the performance of the Bag of Words method. There are multiple steps that you need to follow. \n",
    "\n",
    "**Clean up irregularities:** Text data often as many inconsistencies that will cause algorithms trouble. Computers are very literal by default. Apple with just an uppercase A, APPLE all in uppercase letters,or ApPLe with a mixture of uppercase and lowercase letters will all be counted separately. We can change the text so that all three versions of Apple here will be counted as the same word, by either changing all words to uppercase or to lower case. \n",
    "\n",
    "Punctuation can also cause problems. The basic approach is to deal with this is to remove everything that isn't a standard numberor letter. However, sometimes punctuation is meaningful. In the case of Twitter, @Apple denotes a message to Apple,and #Apple is a message about Apple. For now, we will remove all punctuation, so @Apple,Apple with an exclamation point etc will all count as just Apple. \n",
    "\n",
    "\n",
    "**Remove unhelpful terms: ** Many words are frequently used but are only meaningful in a sentence. These are called stop words. Examples are _the, is, at, and which_ etc. It's unlikely that these words will improve the machine learning prediction quality, so we want to remove them to reduce the size of the data. There are some potential problems with this approach. Sometimes, two stop words taken together have a very important meaning. For example, \"The Who\"-- which is a combination of two stopwords-- is actually the name of a band. By removing the stop words, we remove both of these words, but The Who might actually have a significant meaning for the prediction task. So while removing stop words sometimes is not helpful, it generally is a very helpful preprocessing step.\n",
    "\n",
    "\n",
    "**Stemming: ** This step represents words with different endings as the same word. Following words, argue, argued, argues, and arguing probably do not need to draw a distinction. They could all be represented by a common stem, argu. The algorithmic process of performing this reduction is called stemming. There are many ways to approach the problem. One approach is to build a database of words and their stems. A pro is that this approach handles exceptions very nicely,since we have defined all of the stems. However, it won't handle new words at all,since they are not in the database. This is especially bad for problems where we're using data from the internet,since we have no idea what words will be used.\n",
    "\n",
    "A different approach is to write a rule-based algorithm. In this approach, if a word ends in things like ed, ing, or ly we would remove the ending. A pro of this approach is that it handles new or unknown words well. However, there are many exceptions,and this approach would miss all of these. Words like child and children would be considered different,but it would get other plurals, like dog and dogs. This second approach is widely popular and is called the Porter Stemmer, designedby Martin Porter in 1980, and it's still used today. As a real example from the data set, the phrase \"by far the best customer care service Ihave ever received\" has three words that would be stemmed-- customer, service,and received. The \"er\" would be removed in customer,the \"e\" would be removed in service,and the \"ed\" would be removed in received.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R's packages provide easy-to-use functions for the most common tasks. Load the data set tweets.csv. Since we're working with text data here, we need one extra argument, which is stringsAsFactors=FALSE. You'll always need to add this extra argument whenworking on a text analytics problemso that the text is read in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1181 obs. of  2 variables:\n",
      " $ Tweet: chr  \"I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore\" \"iOS 7 is so fricking smooth & beautiful!! #ThanxApple @Apple\" \"LOVE U @APPLE\" \"Thank you @apple, loving my new iPhone 5S!!!!!  #apple #iphone5S pic.twitter.com/XmHJCU4pcb\" ...\n",
      " $ Avg  : num  2 2 1.8 1.8 1.8 1.8 1.8 1.6 1.6 1.6 ...\n"
     ]
    }
   ],
   "source": [
    "tweets = read.csv(\"C:/DSA/DataMiningAndInfoRetrieval/datasets/tweets.csv\", stringsAsFactors=FALSE)\n",
    "str(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,181 observations of two variables called Tweet and Avg(average sentiment score). Lets see if we can detect any negative sentiment in the tweets. Let's define a new variable in the dataset called Negative. tweets$Negative will be equal to true if the average sentiment score is less than or equal to negative 1 and will be equal to false if the average sentiment score is greater than negative 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "FALSE  TRUE \n",
       "  999   182 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dependent variable\n",
    "\n",
    "tweets$Negative = as.factor(tweets$Avg <= -1)\n",
    "\n",
    "table(tweets$Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 182 of the 1,181 tweets, or about 15%, are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/saikr/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'SnowballC' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\saikr\\AppData\\Local\\Temp\\RtmpS4Q75g\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "# install.packages(\"tm\",repo=\"https://cran.mtu.edu/\")\n",
    "install.packages(\"SnowballC\",repo=\"https://cran.mtu.edu/\")\n",
    "library(tm)\n",
    "library(SnowballC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pre-process the text data, we'll be using the **tm** text mining package. We also need the SnowballC package which will help in using the tm package. tm package introduces the concept of corpus. A corpus is a collection of documents. We'll need to convert our tweets into a corpus for pre-processing. tm can create a corpus in many different ways, but we'll create it from the tweet column of our data frame using two functions, Corpus and VectorSource. We'll call our corpus \"corpus\" and then use the Corpus and the VectorSource functions called on the tweets variable of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus created can be verified by typing corpus and verify that the corpus has 1,181 textdocuments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<SimpleCorpus>>\n",
       "Metadata:  corpus specific: 1, document level (indexed): 0\n",
       "Content:  documents: 1181"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create corpus\n",
    " \n",
    "corpus = Corpus(VectorSource(tweets$Tweet))\n",
    "\n",
    "\n",
    "# Look at corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that the documents match our tweetsby using double brackets.So type corpus[[1]].This shows us the first tweet in our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<PlainTextDocument>>\n",
       "Metadata:  7\n",
       "Content:  chars: 107149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT NOTE: If you are using the latest version of the tm package, you will #need to run the following line \n",
    "# before continuing (it converts corpus to a Plain #Text Document). \n",
    "\n",
    "corpus = tm_map(corpus, PlainTextDocument)\n",
    "\n",
    "corpus[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start pre-processing our data.Pre-processing is easy in tm.Each operation, like stemming or removing stop words,can be done with one line in R, wherewe use the tm_map function.Let's try it out by changing all of the text in our tweetsto lowercase.To do that, we'll replace our corpuswith the output of the tm_map function, wherethe first argument is the name of our corpusand the second argument is what we want to do.In this case, tolower.tolower is a standard function in R,and this is like when we pass mean to the tapply function.We're passing the tm_map functiona function to use on our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "\n",
    "Let's see what that did by looking at our first tweetagain.Go ahead and hit the up arrow twice to get backto corpus[[1]] and now we can see that all of our letters arelowercase.Now let's remove all punctuation.This is done in a very similar way,except this time we give the argumentremovePunctuation instead of tolower.Hit the up arrow twice, and in the tm_map function,delete tolower, and type removePunctuation.Let's see what this did to our first tweet again.Now the comma after \"say\", the exclamation point after\"received\", and the @ symbols before \"Apple\" are all gone.Now we want to remove the stop words in our tweets.tm provides a list of stop words for the English language.We can check it out by typing stopwords(\"english\") [1:10].We see that these are words like \"I,\"\"me,\" \"my,\" \"myself,\" et cetera.Removing words can be done with the removeWords argumentto the tm_map function, but we need one extra argumentthis time-- what the stop words are that we want to remove.We'll remove all of these English stop words,but we'll also remove the word \"apple\"since all of these tweets have the word \"apple\"and it probably won't be very useful in our predictionproblem.So go ahead and hit the up arrow to get backto the tm_map function, delete removePunctuation and, instead,type removeWords.Then we need to add one extra argument, c(\"apple\").This is us removing the word \"apple.\"And then stopwords(\"english\").So this will remove the word \"apple\"and all of the English stop words.Let's take a look at our first tweetagain to see what happened.Now we can see that we have significantly fewer words, onlythe words that are not stop words.Lastly, we want to stem our document with the stemDocumentargument.Go ahead and scroll back up to the removePunctuation,delete removePunctuation, and type stemDocument.If you hit Enter and then look at the first tweet again,we can see that this took off the ending of \"customer,\"\"service,\" \"received,\" and \"appstore.\"In the next video, we'll investigate our corpusand prepare it for our prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
