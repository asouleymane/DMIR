{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Loading Text Search in Python Whoosh\n",
    "\n",
    "\n",
    "## OUTLINE\n",
    " 1. [Whoosh](#Whoosh_text)\n",
    " 1. [Task at hand](#task)\n",
    " 1. [Buiding our Whoosh Schema](#build_it)\n",
    " 1. [Loading Data](#load_it)\n",
    " 1. [Executing Queries, Google-lite...very very lite](#search_me) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='Whoosh_text' ></a>\n",
    "\n",
    "## Whoosh\n",
    "\n",
    "Whoosh was started as a quick and dirty search server for the online documentation of the Houdini 3D animation software package. \n",
    "Side Effects Software generously allowed the code to be open source, in case it might be useful to anyone else who needs a very flexible or pure-Python search engine (or both!).\n",
    "\n",
    "  * Whoosh is fast, but uses only pure Python, so it will run anywhere Python runs, without requiring a compiler.\n",
    "  * By default, Whoosh uses the Okapi BM25F ranking function, but like most things the ranking function can be easily customized.\n",
    "  * Whoosh creates fairly small indexes compared to many other search libraries.\n",
    "  * All indexed text in Whoosh must be unicode.\n",
    "  * Whoosh lets you store arbitrary Python objects with indexed documents.\n",
    "\n",
    "### What is Whoosh?\n",
    "\n",
    "Whoosh is a fast, pure Python search engine library.\n",
    "\n",
    "The primary design impetus of Whoosh is that it is pure Python. \n",
    "You should be able to use Whoosh anywhere you can use Python, no compiler or Java required.\n",
    "\n",
    "Like one of its ancestors, Lucene, Whoosh is not really a search engine, itâ€™s a programmer library for creating a search engine.\n",
    "\n",
    "Practically no important behavior of Whoosh is hard-coded. \n",
    "Indexing of text, the level of information stored for each term in each field, parsing of search queries, the types of queries allowed, scoring algorithms, etc. are all customizable, replaceable, and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "For this lab, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously read about the _`book`_ data and you have seen the data used for a corpus in a PostgreSQL full text search.\n",
    "Now, we are going to walk through the similar process to build the search engine in pure Python.\n",
    "The process will take very little time and the useability of the full text search is multiplied by degree of heterogeneous data that can be integrated with the full text search.\n",
    "\n",
    "Throughout these steps, try to recognize the similarities to the PostgreSQL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content, tags)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this lab, we have created a small folder of a few books under the `lab/` folder:\n",
    "\n",
    "```Bash\n",
    "[scottgs@metal labs]$ ls book_lite/\n",
    "acts.txt  numbers.txt  romans.txt\n",
    "```\n",
    "We will create the _whoosh_ index files in the folder, then ingest the files.\n",
    "\n",
    "To load the data, a python script with follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"book_lite\", schema)\n",
    "\n",
    "# Get a writer form the created index in \n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os,os.path\n",
    "#from whoosh import index\n",
    "#this clears the existing index in the directory\n",
    "#ix = index.create_in(\"book_lite\",schema)\n",
    "# Get a writer from the created index\n",
    "#writer = ix.writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This should look familiar!\n",
    "\n",
    "Note the subtle changes.\n",
    "You should now be able to adapt code such as provided (found) in the PostgreSQL to recursive parsing into the a new API for indexing or other file / textual processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  book_lite\n",
      "root =  book_lite\n",
      "Processing File: book_lite/acts.txt\n",
      "Indexed:  book_lite/acts.txt\n",
      "Processing File: book_lite/numbers.txt\n",
      "Indexed:  book_lite/numbers.txt\n",
      "Processing File: book_lite/romans.txt\n",
      "Indexed:  book_lite/romans.txt\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "recursing into  MAIN.tmp\n",
      "Processing folder:  MAIN.tmp\n",
      "root =  book_lite/MAIN.tmp\n",
      "Unhandled File\n"
     ]
    }
   ],
   "source": [
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r') as infile:\n",
    "        content=infile.read()\n",
    "        writer.add_document(filename=fname, content=content)\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "        # Recurse into subfolders\n",
    "        for d in dirs:\n",
    "            print(\"recursing into \",d)\n",
    "            processFolder(writer,d)\n",
    "\n",
    "# Functions defined,  get the party started:\n",
    "processFolder(writer,\"book_lite\")\n",
    "writer.commit() # save changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "## Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'filename': 'book_lite/acts.txt'}>\n",
      "<Hit {'filename': 'book_lite/numbers.txt'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"abode\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'filename': 'book_lite/romans.txt'}>\n",
      "<Hit {'filename': 'book_lite/acts.txt'}>\n",
      "<Hit {'filename': 'book_lite/numbers.txt'}>\n"
     ]
    }
   ],
   "source": [
    "q = qp.parse(u\"judged OR power\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'filename': 'book_lite/acts.txt'}>\n"
     ]
    }
   ],
   "source": [
    "q = qp.parse(u\"wealth\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'filename': 'book_lite/romans.txt'}>\n",
      "<Hit {'filename': 'book_lite/acts.txt'}>\n"
     ]
    }
   ],
   "source": [
    "q = qp.parse(u\"mightest\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = qp.parse(u\"weak AND powerless\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'filename': 'book_lite/romans.txt'}>\n",
      "<Hit {'filename': 'book_lite/numbers.txt'}>\n",
      "<Hit {'filename': 'book_lite/acts.txt'}>\n"
     ]
    }
   ],
   "source": [
    "q = qp.parse(u\"strong AND powerful\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
