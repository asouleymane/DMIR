{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is an important step in the use of unstructured text document for any type of data mining, information retrieval, or text analytics.\n",
    "This lab walks through the use of the Python Natural Language Toolkit (NLTK) to discuss the tools available for text preprocessing.\n",
    "Specifically, we are looking at the concepts of\n",
    "  1. Stop Words\n",
    "  1. Stemming\n",
    "  1. Lemmatization\n",
    "  \n",
    "In the labs after this, these things will be automatically handled for us as we build information retrieval.\n",
    "However, these are still key concepts to see in action.\n",
    "You will see them again as we continue to move forward with our text analytics in future modules.\n",
    "\n",
    "#### <span style=\"background:yellow\">As you continue to work with text processing, contemplate the discussion board question!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Text documents often contain many occurrences of the same word. \n",
    "For example, in a document written in _English_, words such as _a_, _the_, _of_, and _it_ likely occur very frequently. \n",
    "When classifying a document based on the number of times specific words occur in the text document, \n",
    "these words can lead to biases, especially since they are generally common in **all** text documents you might want to classify. \n",
    "As a result, the concept of [_stop words_](https://en.wikipedia.org/wiki/Stop_words) was invented. \n",
    "Basically, these words are the most commonly occurring words that should be removed during the tokenization process in order to improve subsequent text analytics efforts. \n",
    "\n",
    "We can easily specify that the __English__ stop words should be excluded during tokenization by using the `stop_words`. \n",
    "Note, _stop word_ dictionaries for other languages, or even specific domains, exist and can be used instead. \n",
    "We demonstrate the removal of stop words by using a `CountVectorizer` in the following simple example.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(analyzer='word',lowercase=True)\n",
    "# sample sentence to tokenize\n",
    "my_text= \"This module introduced many concepts in text analysis\"\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english',lowercase=True)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['this', 'module', 'introduced', 'many', 'concepts', 'in', 'text', 'analysis']\n",
      "\n",
      "Tokenization (with Stop words):\n",
      "['module', 'introduced', 'concepts', 'text', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Define our vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)\n",
    "\n",
    "# Sample sentence to tokenize\n",
    "my_text = 'This module introduced many concepts in text analysis.'\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "pp.pprint(tk_func1(my_text))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Tokenization (with Stop words):')\n",
    "pp.pprint(tk_func2(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "--- \n",
    "## Stemming\n",
    "\n",
    "\n",
    "We have looked at the removal of redundant or unimportant words, i.e., _stop words_. \n",
    "However, an issue still exist because of different word forms of the same base term; for example compute, computer, computed, and computing. \n",
    "The process of changing words back to their root term, or basic form (by removing prefixes and suffixes) so that token frequencies match the use of the root token rather than being spread across multiple similar tokens is known as [stemming](https://en.wikipedia.org/wiki/Stemming). \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming, is the _Porter Stemmer_, which was originally published in 1980 by Martin Porter. \n",
    "An improved version was released in 2000, which fixed a number of errors. \n",
    "NLTK includes the Porter Stemmer.\n",
    "This is used by creating a special function that tokenizes text documents and then passes this function as an argument to the `CountVectorizer` via the `tokenizer` attribute. \n",
    "By performing stemming inside this tokenize method, we can return a set of tokens for a document that have been stemmed. \n",
    "In the following code cell, we use a custom `tokenize` method that first builds a list of tokens by using nltk, and then maps the Porter Stemmer to the list of tokens to generate a stemmed list.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull down some data to aid in text analytics\n",
    "\n",
    "Uncomment this cell. \n",
    "Run this cell once to download the NLTK datasets. \n",
    "Once the data is downloaded you can comment out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using in a local install, you will need to install the NLTK data, ~12GB\n",
    "```\n",
    "nltk.download('all')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "new_text = \"It is important to be very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. \n",
    "By inflected it means to change the form of a word to express a particular grammatical function or attribute, typically tense, mood, person, number, case, and gender.\n",
    "\n",
    "In computational linguistics, lemmatization is the algorithmic process of determining the lemma for a given word. \n",
    "The process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence requiring, for example, knowledge of the grammar of a language.\n",
    "\n",
    "In many languages, words appear in several inflected forms. \n",
    "For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. \n",
    "The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. \n",
    "\n",
    "Lemmatization is closely related to stemming. \n",
    "The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. \n",
    "However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to understand the difference between **Stemming** and **Lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem going: go\n",
      "Stem gone: gone\n",
      "Stem goes: goe\n",
      "Stem went: went\n",
      "\n",
      "\n",
      "Without context\n",
      "Lemmatise going: going\n",
      "Lemmatise gone: gone\n",
      "Lemmatise goes: go\n",
      "Lemmatise went: went\n",
      "\n",
      "\n",
      "With context\n",
      "Lemmatise going: go\n",
      "Lemmatise gone: go\n",
      "Lemmatise goes: go\n",
      "Lemmatise went: go\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "print(\"Stem %s: %s\" % (\"going\", stemmer.stem(\"going\")))\n",
    "print(\"Stem %s: %s\" % (\"gone\", stemmer.stem(\"gone\")))\n",
    "print(\"Stem %s: %s\" % (\"goes\", stemmer.stem(\"goes\")))\n",
    "print(\"Stem %s: %s\" % (\"went\", stemmer.stem(\"went\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Without context\")\n",
    "print(\"Lemmatise %s: %s\" % (\"going\", lemmatizer.lemmatize(\"going\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"gone\", lemmatizer.lemmatize(\"gone\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"goes\", lemmatizer.lemmatize(\"goes\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"went\", lemmatizer.lemmatize(\"went\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"With context\")\n",
    "print(\"Lemmatise %s: %s\" % (\"going\", lemmatizer.lemmatize(\"going\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"gone\", lemmatizer.lemmatize(\"gone\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"goes\", lemmatizer.lemmatize(\"goes\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"went\", lemmatizer.lemmatize(\"went\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the stemming process does not generate a real word, but a root form. \n",
    "On the other side, the lemmatizer generates real words, \n",
    "but without contextual information it is not able to distinguish between nouns and verbs. \n",
    "Hence the lemmatization process doesn’t change the word. \n",
    "\n",
    "The context is provided by the POS tag (\"v\" for verb in this example). \n",
    "We cannot specify POS tag everytime in order to lemmatize words in a text. \n",
    "NLTK generates POS tags automatically, using a simple function `pos_tag()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "s = \"This is a simple sentence\"\n",
    "tokens = word_tokenize(s) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) \n",
    " \n",
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `pos_tag()` function generates keywords for each word in the text. \n",
    "The outputs 'DT', 'VBZ' etc represents parts of speech. \n",
    "The output is turn used by NLTK to refer the tokens against the set of tags from the [Penn Treebank project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point\n",
    "Stop Words, Stemming, Lemmatization are important pre-processing steps in text analytics applications. \n",
    "You can leverage the off-the-shelf solutions offered by NLTK into yout text analysis applications.\n",
    "Additionally, many a code libraries and applications that perform more advanced text analytical processes incorporate these techniques in them by default.\n",
    "\n",
    "Below are some practice coding for you to experiment with the NLTK functionality above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StringAction = \"We are meeting\"\n",
    "StringNoun =  \"We had a meeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "'We are meeting':\n",
      "\"'['meeting']':\"\n",
      "  ... vs ...  \n",
      "Tokenization:\n",
      "'We had a meeting':\n",
      "\"'['meeting']':\"\n"
     ]
    }
   ],
   "source": [
    "# 1) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the tokenizer with the english stop words\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Building the vectorizing tokenizer\n",
    "cv = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "tk_function = cv.build_analyzer()\n",
    "\n",
    "# Print out the comparison of stop-word enabled tokenizing the two variables\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "print(\"'{}':\".format(StringAction))\n",
    "# -------- EDIT NEXT LINE --------\n",
    "pp.pprint(\"'{}':\".format(tk_function(StringAction)))\n",
    "\n",
    "print(\"  ... vs ...  \")\n",
    "\n",
    "print('Tokenization:')\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "# -------- EDIT NEXT LINE --------\n",
    "pp.pprint(\"'{}':\".format(tk_function(StringNoun)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We are meeting':\n",
      "We\n",
      "are\n",
      "meet\n",
      "  ... vs ...  \n",
      "'We had a meeting':\n",
      "We\n",
      "had\n",
      "a\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "# 2) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the stemmer with the english stop words\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "### Add your code below to parse and stem the two variables\n",
    "\n",
    "Action_tokens = nltk.word_tokenize(StringAction)\n",
    "Action_tokens = [token for token in Action_tokens if token not in string.punctuation]\n",
    "\n",
    "Noun_tokens = nltk.word_tokenize(StringNoun)\n",
    "Noun_tokens = [token for token in Noun_tokens if token not in string.punctuation]\n",
    "\n",
    "print(\"'{}':\".format(StringAction))\n",
    "for w in Action_tokens:\n",
    "    print(stemmer.stem(w))\n",
    "\n",
    "print(\"  ... vs ...  \")\n",
    "\n",
    "\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "for w in Noun_tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "----------------------------------------\n",
      "'We are meeting':\n",
      "meeting\n",
      "  ... vs ...  \n",
      "'We had a meeting':\n",
      "meeting\n"
     ]
    }
   ],
   "source": [
    "# 3) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the lemmatization with the english stop words\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "### Add your code below to parse and lemmatize the two variables\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Action_tokens = [token for token in word_tokenize(StringAction.lower()) if token not in stop]\n",
    "Noun_tokens = [token for token in word_tokenize(StringNoun.lower()) if token not in stop]\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "print(40*'-')\n",
    "print(\"'{}':\".format(StringAction))\n",
    "for w in Action_tokens:\n",
    "    print(lemmatizer.lemmatize(w))\n",
    "\n",
    "print(\"  ... vs ...  \")\n",
    "\n",
    "\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "for w in Noun_tokens:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing in Full Text Search\n",
    "\n",
    "Now that we have seen these concepts in isolation, lets revisit the PostgreSQL Full Text Search.\n",
    "\n",
    "Specifically, I have loaded a new table that breaks each document from the book in the lab into individual lines.  \n",
    "You can review the load process for this data in [this notebook](./Load_BookLines.ipynb).\n",
    "The notebook include a few queries, showing how the loaded lines may be a little more useful that just document matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "dsa_ro=# select count(*),sum(length(line)) from ir.booklines;\n",
    " count |   sum   \n",
    "-------+---------\n",
    " 31259 | 4315223\n",
    "(1 row)\n",
    "```\n",
    "\n",
    "#### 31K lines\n",
    "\n",
    "#### Looking at a randome line that was added:\n",
    "\n",
    "```SQL\n",
    "dsa_ro=# \\x \n",
    "Expanded display is on.\n",
    "dsa_ro=# select * from ir.booklines where id = 34;\n",
    "-[ RECORD 1 ]-+-------------------------------------------\n",
    "id            | 34\n",
    "name          | ./book/zeph.txt\n",
    "line_no       | 34\n",
    "line          | 2:14: And flocks shall lie down in the midst of her, \n",
    "                all the beasts of the nations: both the cormorant and \n",
    "                the bittern shall lodge in the upper lintels of it; \n",
    "                their voice shall sing in the windows; desolation shall \n",
    "                be in the thresholds: for he shall uncover the cedar work.\n",
    "line_tsv_gin  | '14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 \n",
    "                'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 \n",
    "                'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 \n",
    "                'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "line_tsv_gist | '14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 \n",
    "                'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 \n",
    "                'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 \n",
    "                'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the line \n",
    "\n",
    "    2:14: And flocks shall lie down in the midst of her, \n",
    "    all the beasts of the nations: both the cormorant and \n",
    "    the bittern shall lodge in the upper lintels of it; \n",
    "    their voice shall sing in the windows; desolation shall \n",
    "    be in the thresholds: for he shall uncover the cedar work.\n",
    "\n",
    "Is tokenized into **_text search vector_ (tsv)**: \n",
    "```\n",
    "'14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 \n",
    "'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 \n",
    "'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 \n",
    "'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "```\n",
    "\n",
    "Lets compare this line to the Python tokenizing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:14: And flocks shall lie down in the midst of her, all the beasts of the nations: both the cormorant and  the bittern shall lodge in the upper lintels of it;  their voice shall sing in the windows; desolation shall  be in the thresholds: for he shall uncover the cedar work.\n"
     ]
    }
   ],
   "source": [
    "line = \"2:14: And flocks shall lie down in the midst of her, \" + \\\n",
    "    \"all the beasts of the nations: both the cormorant and  \" + \\\n",
    "    \"the bittern shall lodge in the upper lintels of it;  \" + \\\n",
    "    \"their voice shall sing in the windows; desolation shall  \" + \\\n",
    "    \"be in the thresholds: for he shall uncover the cedar work.\"\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare processing of the line:\n",
    "\n",
    "In the cell below, use each of the Python techniques above to process the `line` variable.\n",
    "Then, answer the questions in the cells below the code to compare and contrast the Python methods versus the apparenet techniques applied by PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2:14: and flocks shall lie down in the midst of her, all the beasts of the nations: both the cormorant and  the bittern shall lodge in the upper lintels of it;  their voice shall sing in the windows; desolation shall  be in the thresholds: for he shall uncover the cedar work.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stop words\n",
      "----------------------------------------\n",
      "['2:14', ':', 'flocks', 'shall', 'lie', 'midst', ',', 'beasts', 'nations', ':', 'cormorant', 'bittern', 'shall', 'lodge', 'upper', 'lintels', ';', 'voice', 'shall', 'sing', 'windows', ';', 'desolation', 'shall', 'thresholds', ':', 'shall', 'uncover', 'cedar', 'work', '.']\n",
      "\n",
      " After Stemming\n",
      "----------------------------------------\n",
      "['2:14', ':', 'flock', 'shall', 'lie', 'midst', ',', 'beast', 'nation', ':', 'cormor', 'bittern', 'shall', 'lodg', 'upper', 'lintel', ';', 'voic', 'shall', 'sing', 'window', ';', 'desol', 'shall', 'threshold', ':', 'shall', 'uncov', 'cedar', 'work', '.']\n",
      "\n",
      " After Lemmatization\n",
      "----------------------------------------\n",
      "['2:14', ':', 'flock', 'shall', 'lie', 'midst', ',', 'beast', 'nation', ':', 'cormorant', 'bittern', 'shall', 'lodge', 'upper', 'lintel', ';', 'voice', 'shall', 'sing', 'window', ';', 'desolation', 'shall', 'threshold', ':', 'shall', 'uncover', 'cedar', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 1) Add code to process the line variable with \n",
    "# Stop Word tokenization, Stemming, and Lemmatization\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "tokens = [token for token in word_tokenize(line.lower()) if token not in stop]\n",
    "print(\"After removing stop words\")\n",
    "print(40*'-')\n",
    "print(tokens)\n",
    "\n",
    "stems=[]\n",
    "for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "print(\"\\n After Stemming\")\n",
    "print(40*'-')\n",
    "print(stems)\n",
    "\n",
    "lemmas=[]\n",
    "print(\"\\n After Lemmatization\")\n",
    "print(40*'-')\n",
    "for w in tokens:\n",
    "    lemmas.append(lemmatizer.lemmatize(w))\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2) Identify the differences and/or similarities in the \n",
    "# remove of stop words from the `line` variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3) Identify the differences and/or similarities in the \n",
    "# stemming of the `line` variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4) Identify the differences and/or similarities in the \n",
    "# lemmatization of the `line` variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
