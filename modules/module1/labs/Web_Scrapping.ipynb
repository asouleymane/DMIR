{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from web\n",
    "\n",
    "With the boom of internet there is so much data lying in the web in the form of websites. \n",
    "There are many ways to extract data from the web. APIs are probably the best way to extract data from a website. \n",
    "Most of the big websites like Twitter, Facebook, amazon, New York Times provide APIs to access their data.But not all websites have an API. \n",
    "Some websites don't provide one because of privacy concerns or they lack technical knowledge to provide one. \n",
    "\n",
    "Web scraping is a technique of extracting information from websites. \n",
    "It focuses on the transformation of unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n",
    "\n",
    "Python has rich eocsystem to scrape data from web and is easy to use. \n",
    "The library ‘BeautifulSoup’ assists this task.\n",
    "\n",
    "#### LIbraries used\n",
    "\n",
    "**`requests`**: \n",
    "This library is used for fetching data from web pages. \n",
    "[Click here for documentation](http://docs.python-requests.org/en/master/)\n",
    "\n",
    "**`BeautifulSoup`**: \n",
    "Use this library to extract tables, lists, paragraph from html web pages. \n",
    "It also allows filters to extract information from web pages. \n",
    "[Click here for documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the library to query a website\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the url\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_World_Series_champions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open website URL and return the html to the variable 'response'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import Beautiful soup library to access functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we get from web is typically html content. \n",
    "We can read the content of the server's response. \n",
    "Below, when a BeautifulSoup object is create from an html response, we explicitly reference the text format(`response.text`). \n",
    "Because the default encoding format is 'UTF-8' as shown below. \n",
    "[Click here for documentation](http://docs.python-requests.org/en/master/user/quickstart/#response-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse the html in the 'response' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prettify function to print the data in nested html structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the table which has list of all baseball world series champions. This table should be present in one of the html tags. Work with the tags to extract data present in them.  \"**soup.tag**\": will return the content between opening and closing tag including tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return string within given tag \n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify the html tag**: The data is in a table. You can use inspect element option when you right click the mouse to identify the tag which has the data. \n",
    "\n",
    " * [Additional guide on webpage inspection](../../../datasets/AnalyzingHTMLwithTheWebInspector.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the right table:** As we are seeking a table to extract information about baseball champions, we should identify the right table first. Let’s write the command to extract information within all table tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tables=soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to identify the right table, we will use attribute “class” of table and use it to filter the right table. In chrome, you can check the class name by right click on the required table of web page –> Inspect element –> Copy the class name OR go through the output of above command find the class name of right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "right_table=soup.find('table', class_='wikitable sortable plainrowheaders')\n",
    "right_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate lists\n",
    "Year=[]\n",
    "Winning_team=[]\n",
    "Winning_Manager=[]\n",
    "Games=[]\n",
    "Losing_team=[]\n",
    "Losing_Manager=[]\n",
    "Ref=[]\n",
    "\n",
    "# skip first iteration as we dont need headers \n",
    "for row in right_table.findAll(\"tr\")[1:]: \n",
    "    game_year=row.findAll('th') # To store game year which is in <th> tag\n",
    "    cells = row.findAll('td') # To store all other details\n",
    "    if len(cells)>2: # Only extract information if there is table body not heading\n",
    "        Year.append(game_year[0].find(text=True))\n",
    "        Winning_team.append(cells[0].find(text=True))\n",
    "        Winning_Manager.append(cells[1].find(text=True))\n",
    "        Games.append(cells[2].find(text=True))\n",
    "        Losing_team.append(cells[3].find(text=True))\n",
    "        Losing_Manager.append(cells[4].find(text=True))\n",
    "        Ref.append(cells[5].find(text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the information to DataFrame:\n",
    "Here, we need to iterate through each row (tr) and then assign each element of tr (td) to a variable and append it to a list. Let’s first look at the HTML structure of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(Year,columns=['Year'])\n",
    "df['Winning_team']=Winning_team\n",
    "df['Winning_Manager']=Winning_Manager\n",
    "df['Games']=Games\n",
    "df['Losing_team']=Losing_team\n",
    "df['Losing_Manager']=Losing_Manager\n",
    "df['Ref']=Ref\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
