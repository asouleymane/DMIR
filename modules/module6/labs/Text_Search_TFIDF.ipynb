{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Loading Text Search in Python Whoosh using TFIDF\n",
    "\n",
    "\n",
    "## OUTLINE\n",
    "\n",
    " 1. [Task at hand](#task)\n",
    " 1. [Buiding our Whoosh Schema](#build_it)\n",
    " 1. [Loading Data](#load_it)\n",
    " 1. [Scoring](#Scoring)\n",
    " 1. [Executing Queries, Google-lite...very very lite](#TFIFD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "For this lab, we are going to revist the IR_with_Python_Whoosh lab in module 5 which walks us through the process of creating full text search capability within Python. In addition to that we are going to incorporate a scoring technique called TFIDF for ranking documents based on TFIDF scores of the terms occuring in the documents. \n",
    "\n",
    "We will walk through the process to build the search engine in Python using whoosh. We will compare the serach results with and without TFIDF method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, structure the retrieval system by defining a storage schema.\n",
    "\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content, tags)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                line_num=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer(),stored=True)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "The books are in the folder called books in `lab/` folder:\n",
    "\n",
    "Create the _whoosh_ index files in the folder, then ingest the files.\n",
    "\n",
    "To load the data, a python script with follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"book\", schema)\n",
    "\n",
    "# Get a writer form the created index in \n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    line_no = 1\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip('\\n')\n",
    "            line_no += 1\n",
    "            writer.add_document(filename=fname, line_num=str(line_no),content=line)\n",
    "    print(\"Indexed: \", fname)\n",
    "\n",
    "\n",
    "#     with open(fname, 'r') as infile:\n",
    "#         content=infile.read()\n",
    "#         txt = content.splitlines()\n",
    "#         for line in txt:\n",
    "#             writer.add_document(filename=fname, content=line)\n",
    "#         print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "        # Recurse into subfolders\n",
    "        for d in dirs:\n",
    "            print(\"recursing into \",d)\n",
    "            processFolder(writer,d)\n",
    "\n",
    "# Functions defined,  get the party started:\n",
    "processFolder(writer,\"book\")\n",
    "writer.commit() # save changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "## Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"love\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the results contains at most the first 10 matching documents. To get more results, use the limit keyword:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with ix.searcher() as s:\n",
    "    results = s.search(q, limit=20)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the limit for maximum of 20 results to return and above code will return 19 results. If you want all results, use limit=None. However, setting the limit whenever possible makes searches faster because Whoosh doesn’t need to examine and score every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id='Scoring' ></a>\n",
    "\n",
    "## Scoring\n",
    "\n",
    "Until this point you should be familiar with all the code above. The code cell above illustrates the search results using the vector space model. In coming cells, we will be using a scoring criteria while searching the indexes below. \n",
    "\n",
    "\n",
    "Normally the list of result documents is sorted by score. The whoosh.scoring module contains implementations of various scoring algorithms. The default is [BM25F](https://en.wikipedia.org/wiki/Okapi_BM25). You can set the scoring object to use when you create the searcher using the weighting keyword argument: \n",
    "\n",
    "````\n",
    "from whoosh import scoring\n",
    "\n",
    "with myindex.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    ... ````\n",
    "    \n",
    "    \n",
    "A weighting model is a WeightingModel subclass with a scorer() method that produces a “scorer” instance. This instance has a method that takes the current matcher and returns a floating point score.\n",
    "\n",
    "\n",
    "\n",
    "### TF IFD\n",
    "\n",
    "So why do we have to score the terms. Previously, we have simply used the number of times a token (i.e., word, or more generally an n-gram) occurs in a document to classify the document. Even with the removal of stop words, however, this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). An alternative technique that often provides robust improvements in classification accuracy is to employ the frequency of token occurrence, normalized over the frequency with which the token occurs in all documents. In this manner, we give higher weight in the classification process to tokens that are more strongly tied to a particular label. \n",
    "\n",
    "Formally this concept is known as [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf–idf) (or tf-idf). We will use this scoring method to compare the search results with normal vector space model.\n",
    "\n",
    "In below code cell, documents with a better TF-IDF score will appear higher in the search results list. Compare below results with the results of above cell which was using basic vector space model for scoring documents. Read the below documents to understand what TF_IDF nis about and how it is applied in whoosh. \n",
    " \n",
    "\n",
    "-----\n",
    "\n",
    "Reference: \n",
    "\n",
    "- [Scoring and sorting](http://whoosh.readthedocs.io/en/latest/searching.html#scoring-and-sorting)\n",
    "- [TF-IDF](http://www.tfidf.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"love\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the files **\"1samuel.txt\"** and **\"hosea.txt\"** have made it to top 10 while the file **john.txt** which was at position 6 and 7 is pushed down to positions 8 and 9 because of the TFIDF scores because of the ranking based on TDIDF scores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Filtering results\n",
    "\n",
    "You can use the filter keyword argument in search() to specify a set of documents to permit in the results. The argument can be a whoosh.query.Query object, a whoosh.searching.Results object, or a set-like object containing document numbers. The searcher caches filters so if for example you use the same query filter with a searcher multiple times, the additional searches will be faster because the searcher will cache the results of running the filter query. You can also specify a mask keyword argument to specify a set of documents that are not permitted in the results. \n",
    "\n",
    "Lets first look up documents where hate is appearing.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"hate\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below code cell, we are using filter argument to allow John.txt only in the results and mask the word hate. So if you observe the results below, indexes in john.txt have appeared and none of the indexes have hate in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.query import *\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(u\"love\")\n",
    "\n",
    "    # Only show documents in the \"rendering\" chapter\n",
    "    allow_q = Term(\"filename\", \"book/john.txt\")\n",
    "    # Don't show any documents where the \"content\" field contains \"hate\"\n",
    "    restrict_q = Term(\"content\",\"hate\")\n",
    "\n",
    "    results = s.search(user_q, mask=restrict_q, filter=allow_q)      #   \n",
    "    for hit in results:\n",
    "        print(hit[\"filename\"], hit[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Lets put our results into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from whoosh.searching import Hit \n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(u\"love\")\n",
    "    \n",
    "    results = s.search(user_q)\n",
    "    print(\"Total no of matches: \",len(results))\n",
    "    \n",
    "    rank=[]\n",
    "    docnum=[]\n",
    "    score=[]\n",
    "    filenames=[]\n",
    "    lines=[]\n",
    "    line_num=[]\n",
    "    \n",
    "    for i in np.arange(0,10):\n",
    "        rank.append(results[i].rank)\n",
    "        docnum.append(results[i].docnum)\n",
    "        score.append(results[i].score)\n",
    "        filenames.append(results[i]['filename'])\n",
    "        line_num.append(results[i]['line_num'])\n",
    "        lines.append(results[i]['content'])\n",
    "       \n",
    "    df = pd.DataFrame({'filename' : filenames, 'line_num' : line_num, 'line' : lines, 'docnum' : docnum, \\\n",
    "                            'score' : score, 'rank' : rank})\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line_num above is the actual line number in the text file. docnum should be the index number in the whole indexes we have created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
