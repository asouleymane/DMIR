{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Module 4 Lab - Anomaly Detection\n",
    "\n",
    "_Anomaly detection (also outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset - wikipedia.com_\n",
    "\n",
    "Most of the times outliers are a by product of clustering algorithms. Clustering algorithms are not designed to detect outliers but rather to form clusters of data. The accuracy of how well outliers are detected depends on how well clustering algorithms can capture the structures. Sometimes a set of similar abnormal objects are group as a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "Outliers are the samples that are exceptionally far from the mainstream of data. There is no rigid mathematical definition of what constitutes an outlier; determining whether or not an observation is an outlier is ultimately a subjective exercise. There are various methods of outlier detection. Some are graphical such as normal probability plots. Others are model-based.\n",
    "\n",
    "There are two main types of outliers, representative and nonrepresentative. A representative outlier is one that is a correct or valid observation that \"cannot be regarded as unique\". While this type of outlier is considered an extreme value, it should be retained, with special treatment during the analysis stages. A nonrepresentative outlier is one that is an \"incorrect observation\" (i.e., due to an error in data entry, coding, or measurement) or is considered unique because there are no other values like it in the population. Nonrepresentative outliers should be corrected or excluded from the analysis.\n",
    "\n",
    "Predictive modeling techniques can be impacted as a result of presence of outliers. Dealing with outliers in data analysis is one of the important challenges. Detecting outliers and understanding them can lead to interesting findings. Outliers can impact accuracy of predictive models. Detecting outliers and dealing with them is a critical step in data preparation for predictive modeling. Below picture shows how an outlier can impact the overall fit of a linear regression model. Lets discuss how PCA technique can be used for detecting outliers in multivariate datasets. \n",
    "\n",
    "<img src=\"../Images/outlier.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to detect outliers:\n",
    "\n",
    "There are several approaches for detecting Outliers. Charu Aggarwal in his book [Outlier Analysis](http://www.charuaggarwal.net/outlierbook.pdf) classifies Outlier detection models in following groups:\n",
    "\n",
    "**Extreme Value Analysis:** This is the most basic form of outlier detection and only good for 1-dimension data. In these types of analysis, it is assumed that values which are too large or too small are outliers. Z-test and Student’s t-test are examples of these statistical methods. These are good heuristics for initial analysis of data but they don’t have much value in multivariate settings. They can be used as final steps for interpreting outputs of other outlier detection methods.\n",
    "\n",
    "**Probabilistic and Statistical Models:** These models assume specific distributions for data. Then using the expectation-maximization(EM) methods they estimate the parameters of the model. Finally, they calculate probability of membership of each data point to calculated distribution. The points with low probability of membership are marked as outliers.\n",
    "\n",
    "**Linear Models:** These methods model the data into a lower dimensional sub-spaces with the use of linear correlations. Then the distance of each data point to plane that fits the sub-space is being calculated. This distance is used to find outliers. PCA(Principal Component Analysis) is an example of linear models for anomaly detection.\n",
    "\n",
    "**Proximity-based Models:** The idea with these methods is to model outliers as points which are isolated from rest of observations. Cluster analysis, density based analysis and nearest neighborhood are main approaches of this kind.\n",
    "\n",
    "**Information Theoretic Models:** The idea of these methods is the fact that outliers increase the minimum code length to describe a data set. High-Dimensional Outlier Detection: Specifc methods to handle high dimensional sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection Using Principal Component Analysis\n",
    "\n",
    "PCA is a statistical procedure that uses a transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set.\n",
    "\n",
    "Using PCA we can map our dataset with n-dimension (possibly correlated variables) to a k-dimensional sub-space of k uncorrelated components (k<=n).\n",
    "\n",
    "Below are steps for detecting anomalies using PCA:\n",
    "\n",
    "First we map the data set from its original n-dimensional space to k-dimensional subspace using PCA\n",
    "\n",
    "- Calculate the centroid of data points (μ)\n",
    "- Calculate the variance of each component (λ)\n",
    "- Calculate the score of each data point using below formula:\n",
    "\n",
    "<img src=\"../Images/PCA_formula.png\">\n",
    "\n",
    "- Finally, use extreme value analysis methods to find data points with extreme scores\n",
    "\n",
    "You dont have to worry about applying above formula to calculate scores. PCA is alreday implemented. You just have to call the method to find scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"psych\",dependencies = TRUE, repos = c(CRAN=\"https://cran.r-project.org/\"))\n",
    "library(psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we load the diamonds dataset\n",
    "library(ggplot2)\n",
    "data(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the cell to know more about data\n",
    "help(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the categorical variables to dummy variables as PCA doesn't run with character type factors. dummy variables \n",
    "# are explained below with example cde.\n",
    "# Use dummy.code from psych package\n",
    "codedData <- cbind(diamonds,dummy.code(diamonds$cut),dummy.code(diamonds$color),dummy.code(diamonds$clarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: ** [Dummy coding the data](http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/)\n",
    "\n",
    "Below piece of code illustrates how dummy coding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate example dataframe with character column\n",
    "example <- as.data.frame(c(\"A\", \"A\", \"B\", \"F\", \"C\", \"G\", \"C\", \"D\", \"E\", \"F\"))\n",
    "names(example) <- \"strcol\"\n",
    "\n",
    "# For every unique value in the string column, create a new 1/0 column\n",
    "# This is what Factors do \"under-the-hood\" automatically when passed to function requiring numeric data\n",
    "for(level in unique(example$strcol)){\n",
    "  example[paste(\"dummy\", level, sep = \"_\")] <- ifelse(example$strcol == level, 1, 0)\n",
    "}\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From output of below str() command, you can see five new columns 11 to 15 are created for the corresponding 5 levels in cut variable. Like wise, seven new columns are created for 7 levels in color factor variable and finally eight new dummy columns are created for the 8 levels in clarity variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(codedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 2, 3, 4 are factors with charaters values. Remove these columns from data as PCA can only work with numeric data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codedData <- codedData[,-c(2,3,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the prcomp method to find Principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr <- prcomp(codedData,center = TRUE, scale. = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scores: ** The positions of each observation in this new coordinate system of principal components are called scores and are calculated as linear combinations of the original variables and the weights aij. For example, the score for the rth sample on the kth principal component is calculated as\n",
    "\n",
    "$$Y_{kr} = a_{k1}x_{k1} + a_{k2}x_{k2} + ... + a_{kp}x_{kp}$$\n",
    "\n",
    "**Reference: ** [PCA tutorial](http://strata.uga.edu/software/pdf/pcaTutorial.pdf)\n",
    "\n",
    "These scores are nothing but the principal components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sum all the principal component values for each row which will be score for the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components <- pr$x\n",
    "scores <- rowSums(components)\n",
    "scores[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram and boxplot for the scores calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(scipen=999)\n",
    "par(mfrow=c(2,1));\n",
    "\n",
    "#Create histogram of scores\n",
    "hist(scores,breaks = 100)\n",
    "\n",
    "#Drawing Boxplot\n",
    "boxplot(scores,horizontal = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows have scores in the range of 100 to 500. Check rows with scores > 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(range(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(which(scores>25))\n",
    "diamonds[which(scores>25),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the y value in first row. It clearly suggests y value is incorrect. It looks like the depth value has been incorrectly used for y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Outlier Factor\n",
    "\n",
    "Local outlier factor is another way of finding outliers. Read the below wiki page to learn how this method works. \n",
    "\n",
    "**Reference: ** \n",
    "- [Wiki](https://en.wikipedia.org/wiki/Local_outlier_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LOF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: ** [lofactor()](https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/lofactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(DMwR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # point to the prostate data set in the h2o folder - no need to load h2o in memory yet\n",
    "\n",
    "prostate_df <- read.table(\"/dsa/data/DSA-8630/prostate.txt\",sep=',',header=TRUE)\n",
    "head(prostate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim(prostate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # We don't need the ID field\n",
    "prostate_df <- prostate_df[,-1]\n",
    "summary(prostate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lofactor(), you are just telling the function how many neighbours it should consider by giving a number to k when calculating the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k=5 indicates to pick top 5 outliers\n",
    "outlier.scores <- lofactor(prostate_df, k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the outlier scores are calculated, pick the top 10 outliers with highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliers <- order(outlier.scores, decreasing=T)[1:10]\n",
    "\n",
    "# who are outliers\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Outliers with Plots\n",
    "\n",
    "Next, we show outliers with a biplot of the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n <- nrow(prostate_df)\n",
    "labels <- 1:n\n",
    "labels[-outliers] <- \".\"\n",
    "biplot(prcomp(prostate_df), cex=.8, xlabs=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its up to you to decide if below values are outliers and removed from dataset. It requires domain knowledge or knowledge about the data to make decisions on data. You may want to do more analysis before excluding them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prostate_df[outliers,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Detection in Time Series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(tsoutliers)\n",
    "library(expsmooth)\n",
    "library(fma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in table_a has the historical stock data of different companies. It has following fields Date, Time, Open, High, Low, Close, Volume.  \n",
    "\n",
    "\n",
    "- Date – This provides the date as an integer where 20100527 would represent May 27th, 2010.\n",
    "- Time – This gives the time as an integer where 1426 would represent 2:26PM EST.\n",
    "- Open – The open price.\n",
    "- High – The high price.\n",
    "- Low – The low price.\n",
    "- Close – The close price.\n",
    "- Volume – The trading volume during the interval. Note that it is extremely difficult to get accurate volume information. The volume is adjusted for splits so that the total value of shares traded remains constant even if a split occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=c(\"Date\",\"Time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\")\n",
    "stock = read.csv(\"/dsa/data/DSA-8630/table_a.csv\",header=FALSE)\n",
    "names(stock) = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Close column into a time series data. start and end has the time periods for which the data is available. Since we have the data for everyday (although there are some missing dates) we will use frequency=30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_series <- ts(stock$Close, start=c(1999, 11, 18), end=c(2013, 08, 09), frequency=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stock_series[1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package detects 5 different types of outliers iteratively in time series data:\n",
    "\n",
    "- Additive Outlier (AO)\n",
    "- Innovation Outlier (IO)\n",
    "- Level Shift (LS)\n",
    "- Temporary change (TC)\n",
    "- Seasonal Level Shift (SLS)\n",
    "\n",
    "[Click here for definition of these outliers](https://www.ibm.com/support/knowledgecenter/SS3RA7_15.0.0/com.ibm.spss.modeler.help/ts_outliers_overview.htm)\n",
    "\n",
    "In below cell we are looking for Additive, Level shift and temporarry chnage outliers in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stock_outliers <- tso(stock_series,types = c(\"AO\",\"LS\",\"TC\"),maxit.iloop=10)\n",
    "stock_outliers\n",
    "plot(stock_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "**Have you used apply() before.. Here's something similar but does little more than that** \n",
    "\n",
    "- [sweep()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/sweep.html)\n",
    "\n",
    "**Syntax:** sweep(x, MARGIN, STATS, FUN=\"-\", check.margin=T, ...)\n",
    "\n",
    "\n",
    "sweep is similar to apply() where yu apply a function to each column or row of a dataframe. But sweep is typically used when you operate a matrix by row or by column, and need the flexibility for the input of the operation to have a different value for each row / column. Whether you operate by row(1) or column(2) is defined by MARGIN, the second parameter in below code. So in below piece of code, for each column you will take a value from c(10, 20, 30) which is being defined by STATS parameter and use in the operation \"+\" defined by FUN parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample code explaining what sweep() is doing\n",
    "a = c(130,110,118,112,128)\n",
    "b = c(26,24,25,25,26)\n",
    "c = c(140,155,142,175,170)\n",
    "names=c(\"Weight\",\"Waist\",\"Height\")\n",
    "size = data.frame(a,b,c)\n",
    "names(size)=names\n",
    "print(size)\n",
    "\n",
    "# We are adding values 10, 20, 30 to columns \n",
    "sweep(size, 2, c(10, 20, 30), \"+\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
